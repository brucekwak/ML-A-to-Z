mu
sigma2
sigma
plot(x, dbinom(x,n,p), type="h", ylab="probability")
abline(h=0)
qbinom(0.95, n, p)
qnorm(0.95, mu, sigma)
v1 <- rnorm(5, 0, 1)
v2 <- rnorm(6, 1,1)
v3 <- rnorm(4,0,1.5)
v1
v2
v3
makefact <- stack(list(level1=v1, level2=v2, level3=v3))
makefact
boxplot(makefact$values~makefact$ind)
boxplot(values~ind, data=makefact)
is.factor(makefact$ind)
test <- lm(values ~ ind, data = makefact)
test
summary(test)
anova(test)
x <- rnomrm(10)   # mean = 0, sd = 1 인 정규분포에서 난수 10개 추출
x
x <- rnorm(10)   # mean = 0, sd = 1 인 정규분포에서 난수 10개 추출
x
which( x < -1 | x > 1)
y <- x[ which( x<-1 | x>1 ) ]
y
which( x<-1 | x>1 )
which( x < -1 | x > 1 )
x
x <- rnorm(10)   # mean = 0, sd = 1 인 정규분포에서 난수 10개 추출
x
which(x < -1 | x > 1)
y <- x[ which( x < -1 | x > 1 ) ]
y
x <- seq(1, 3, 0.5)
x
x > 1
y <- c(x>1)
y
-y
x[-y]
-TRUE
-FALSE
TRUE + TRUE
x[y]
x[1]
x[1,2,3]
x[c(1,2,3)]
x[c(0, -1, -1, -1, -1)]
x[c(0, -1, -1)]
x[c(0, -1)]
x[-1]
x
x[-1]
x[c(TRUE, FALSE, TRUE, FALSE)]
x
x[-1]   # 첫 원소릴 제외한 벡터
x <- ㄴㄷㅂ(1, 3, 0.5)
x <- seq(1, 3, 0.5)
y <- c(x>1)
x
y
-y
x[-y]
x[c(0, -1)]
x[c(0)]
x[c(-1, -1)]
x[c(1,1,1)]
x[c(1,2,1)]
x[c(0, -1, -1, -1)]
x[c(0, -1)]
x[-1]   # 첫 원소를 제외한 벡터
x[c(0, -1, -1)]
x[c(-1)]
x[c(-1,-1)]
x[c(1,1)]
x[c(1,2,1,3,2)]
x[c(1,2,1,3,-1)]
x[c(0, 1,2,1,3,-1)]
x <- seq(1, 3, 0.5)
x
y <- c(x>1)
y
-y
x[c(0, -1)]
x[c(1,3,3,3,3,3,3)]
x[c(1,3,2,3,1,3,2,3)]
x[y]
y
x[c(0, 1, 1, 1, 1)]
x[c(FALSE, TRUE, TRUE, TRUE)]
x <- seq(1, 3, 0.5)
x
y <- c(x>1)
y
-y
x[-y]
x[c(0, -1, -1, -1, -1)]
x[c(-1)]
x[c(-1, -1)]
x[c(-2, -2)]
x[c(-2)]
x[c(-2, -4)]
x[ c(0) ]
x[1]
x[2]
x[c(-1, 4)]
abc <- c(13, 23, 24, 13, 1)
x[c( 0, 1, 2)]
x[1]
x[2]
x[0]
###################################
# library
###################################
install.packages("car")
###################################
# library
###################################
install.packages("raster")
###################################
# library
###################################
install.packages("e1071")
library(car)
library(raster)
library(e1071)
library(dplyr)
###################################
# data loading
###################################
data<- read.csv("interarrival.csv",header = TRUE)
###################################
# data loading
###################################
data<- read.csv("interarrival.csv",header = TRUE)
data2 <- filter(data, data$Interarrival_Time_batch>0)
###################################
# scatter plot
###################################
data2$Interarrival_Time_batch_2 <- lag(data2$Interarrival_Time_batch, n=1)
library(dplyr)
###################################
# library
###################################
install.packages("dplyr")
library(dplyr)
data2 <- filter(data, data$Interarrival_Time_batch>0)
View(data2)
data2$Batch_Size2 <- lag(data2$Batch_Size, n=1)
scatterplot(data2$Batch_Size ~data2$Batch_Size2, xlab = "Interarrival_Time_batch", ylab = "Interarrival_Time_batch_2", main = "Interarrival_Time")
View(data2)
hist(data2$Batch_Size, xlab = "Batch_Size", col = "black")
boxplot(data2$Batch_Size, main = "Batch_Size")
quantile(data2$Batch_Size, probs = seq(0,1,0.125), type = 7)
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
paste0("corr", i) <- cor(data2$Interarrival_Time_batch, tmp, method = "pearson")
}
tmp <- lag(data2$Interarrival_Time_batch, n=1)
cor(data2$Interarrival_Time_batch, tmp, method = "pearson")
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
paste0("corr", i) <- cor(data2$Interarrival_Time_batch[i+1:nrow(data2)-i], tmp[i+1:nrow(tmp)-i], method = "pearson")
}
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
paste0("corr", i) <- cor(data2$Interarrival_Time_batch[i+1:nrow(data2)-i], tmp[i+1:length(tmp)-i], method = "pearson")
}
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
paste0("corr", i, sep = "_") <- cor(data2$Interarrival_Time_batch[i+1:nrow(data2)-i], tmp[i+1:length(tmp)-i], method = "pearson")
}
paste0("corr", 1, sep = "_")
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
paste0("corr", i, collapse = "_") <- cor(data2$Interarrival_Time_batch[i+1:nrow(data2)-i], tmp[i+1:length(tmp)-i], method = "pearson")
}
cor(data2$Interarrival_Time_batch[2:nrow(data2)-1], tmp[2:length(tmp)-1], method = "pearson")
tmp
View(data2)
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
paste0("corr", i, collapse = "_") <- cor(data2$Interarrival_Time_batch[i+1:nrow(data2)], tmp[i+1:length(tmp)], method = "pearson")
}
cor(data2$Interarrival_Time_batch[2:nrow(data2)], tmp[2:length(tmp)], method = "pearson")
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
assign(paste0("corr", i),cor(data2$Interarrival_Time_batch[i+1:nrow(data2)], tmp[i+1:length(tmp)], method = "pearson"))
}
tmp
View(data2)
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
data2[i,6]<-cor(data2$Interarrival_Time_batch[i+1:nrow(data2)], tmp[i+1:length(tmp)], method = "pearson"))
}
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
data2[i,6]<-cor(data2$Interarrival_Time_batch[i+1:nrow(data2)], tmp[i+1:length(tmp)], method = "pearson")
}
View(data2)
tmp <- lag(data2$Interarrival_Time_batch, n=1)
cor(data2$Interarrival_Time_batch[1+1:nrow(data2)], tmp[1+1:length(tmp)], method = "pearson")
cor(data2$Interarrival_Time_batch[2:nrow(data2)], tmp[2:length(tmp)], method = "pearson")
###################################
# Correlation
###################################
for(i in 1:20){
tmp <- lag(data2$Interarrival_Time_batch, n=i)
data2[i,6]<-cor(data2$Interarrival_Time_batch[(i+1):nrow(data2)], tmp[(i+1):length(tmp)], method = "pearson")
}
View(data2)
plot(sep(1,20,1)~data[1:20,6])
plot(seq(1,20,1)~data[1:20,6])
View(data2)
plot(seq(1,20,1)~data2[1:20,6])
plot(data2[1:20,6]~seq(1,20,1))
plot(data2[1:20,6]~seq(1,20,1), ylim=(-1,1))
plot(data2[1:20,6]~seq(1,20,1), ylim=v(-1,1))
plot(data2[1:20,6]~seq(1,20,1), ylim=range(-1:1))
pllogis(q, shape, rate = 1, scale = 1/rate,
lower.tail = TRUE, log.p = FALSE)
install.packages("actuar")
pllogis(q, shape, rate = 1, scale = 1/rate,
lower.tail = TRUE, log.p = FALSE)
library(actuar)
pllogis(q, shape, rate = 1, scale = 1/rate,
lower.tail = TRUE, log.p = FALSE)
pllogis(q, shape, rate = 1, scale = 1/rate, lower.tail = TRUE, log.p = FALSE)
?pllogis
rlnorm(20, log(10), log(2.5))
rweibull(36,26.844, 6.2026)
rweibull(36, scale = 26.844, shape = 6.2026)
test <- as.data.frame(rweibull(36, scale = 26.844, shape = 6.2026))
View(test)
colnames(test) <- "p"
test$p <- as.character(test$p)
test$p2 <- substr(test$p, 1,5)
View(test)
test<- test[,2]
test <- as.data.frame(test)
View(test)
test <- as.data.frame(rweibull(40, scale = 26.844, shape = 6.2026))
colnames(test) <- "p"
test$p <- as.character(test$p)
test$p2 <- substr(test$p, 1,6)
View(test)
test$p2 <- as.numeric(test$p2)
test$p3 <- round(test$p2, digit=2)
test<- as.data.frame(test[,3])
test <- as.data.frame(rweibull(40, scale = 26.844, shape = 6.2026))
colnames(test) <- "p"
test$p <- as.character(test$p)
test$p2 <- substr(test$p, 1,6)
test$p2 <- as.numeric(test$p2)
test$p3 <- round(test$p2, digit=2)
setwd('C:/Users/KYY/Desktop/ML_Ato_Z/[0]Data')
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
setwd('C:/Users/KYY/Desktop/ML_Ato_Z/[0]Data')
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
View(dataset)
View(dataset)
dataset = dataset[2:4]
View(dataset)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
View(dataset)
split == FALSE
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(training_set)
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
View(training_set)
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
classifier
prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
prob_pred
prob_pred > 0.5
prob_pred[prob_pred > 0.5]
length(prob_pred[prob_pred > 0.5])
y_pred = ifelse(prob_pred > 0.5, 1, 0)  # test, yes, no
y_pred
y_pred == 1
perf_eval2 <- function(cm){
# True positive rate: TPR (Recall)
TPR <- cm[2,2]/sum(cm[2,])
# Precision
PRE <- cm[2,2]/sum(cm[,2])
# True negative rate: TNR
TNR <- cm[1,1]/sum(cm[1,])
# Simple Accuracy
ACC <- (cm[1,1]+cm[2,2])/sum(cm)
# Balanced Correction Rate
BCR <- sqrt(TPR*TNR)
# F1-Measure
F1 <- 2*TPR*PRE/(TPR+PRE)
return(c(TPR, PRE, TNR, ACC, BCR, F1))
}
# Initialize the performance matrix
perf_mat <- matrix(0, 1, 6)
colnames(perf_mat) <- c("TPR (Recall)", "Precision", "TNR", "ACC", "BCR", "F1")
rownames(perf_mat) <- "Logstic Regression"
# Load dataset
setwd('C:/Users/KYY/Desktop/ML_Ato_Z/[0]Data')
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
input_idx <- c(1, 2)
target_idx <- 3
# Conduct the normalization
dataset_input <- dataset[,input_idx]
dataset_input <- scale(ploan_input, center = TRUE, scale = TRUE)
dataset_target <- dataset[,target_idx]
dataset <- data.frame(dataset_input, dataset_target)
# Split the data into the training/validation sets
set.seed(12345)
trn_idx <- sample(1:nrow(dataset), round(0.7*nrow(dataset)))
dataset_trn <- dataset[trn_idx,]
dataset_tst <- dataset[-trn_idx,]
# Conduct the normalization
dataset_input <- dataset[,input_idx]
dataset_input <- scale(dataset_input, center = TRUE, scale = TRUE)
dataset_target <- dataset[,target_idx]
dataset <- data.frame(dataset_input, dataset_target)
# Split the data into the training/validation sets
set.seed(12345)
trn_idx <- sample(1:nrow(dataset), round(0.7*nrow(dataset)))
dataset_trn <- dataset[trn_idx,]
dataset_tst <- dataset[-trn_idx,]
dataset
View(dataset)
full_lr <- glm(dataset_target ~ ., family=binomial, dataset)
summary(full_lr)
lr_response <- predict(full_lr, type = "response", newdata = ploan_tst)
lr_target <- ploan_tst$ploan_target
lr_predicted <- rep(0, length(lr_target))
lr_predicted[which(lr_response >= 0.5)] <- 1
cm_full <- table(lr_target, lr_predicted)
cm_full
perf_mat[1,] <- perf_eval2(cm_full)
perf_mat
lr_response <- predict(full_lr, type = "response", newdata = ploan_tst)
lr_target <- dataset_tst$dataset_target
lr_predicted <- rep(0, length(lr_target))
lr_predicted[which(lr_response >= 0.5)] <- 1
cm_full <- table(lr_target, lr_predicted)
cm_full
lr_response <- predict(full_lr, type = "response", newdata = dataset_tst)
lr_target <- dataset_tst$dataset_target
lr_predicted <- rep(0, length(lr_target))
lr_predicted[which(lr_response >= 0.5)] <- 1
cm_full <- table(lr_target, lr_predicted)
cm_full
perf_mat[1,] <- perf_eval2(cm_full)
perf_mat
cm = table(test_set[, 3], y_pred)
cm
# Performance Evaluation Function -----------------------------------------
perf_eval2 <- function(cm){
# True positive rate: TPR (Recall)
TPR <- cm[2,2]/sum(cm[2,])
# Precision
PRE <- cm[2,2]/sum(cm[,2])
# True negative rate: TNR
TNR <- cm[1,1]/sum(cm[1,])
# Simple Accuracy
ACC <- (cm[1,1]+cm[2,2])/sum(cm)
# Balanced Correction Rate
BCR <- sqrt(TPR*TNR)
# F1-Measure
F1 <- 2*TPR*PRE/(TPR+PRE)
return(c(TPR, PRE, TNR, ACC, BCR, F1))
}
eval = perf_eval2(cm)
eval
perf_mat <- matrix(0, 1, 6)
colnames(perf_mat) <- c("TPR (Recall)", "Precision", "TNR", "ACC", "BCR", "F1")
rownames(perf_mat) <- "Logstic Regression"
perf_mat[1, ] = perf_eval2(cm)
print(perf_mat)
install.packages('ElemStatLearn')
library(ElemStatLearn)
range(X1)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2), add = TRUE))
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd('C:/Users/KYY/Desktop/ML_Ato_Z/[0]Data')
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Splitting the datset into the Training set and Test set
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
# Fitting Logistic Regression to the Training set
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
# Predicting the Test set results
prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)  # test, yes, no
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Performance Evaluation Function
perf_eval2 <- function(cm){
# True positive rate: TPR (Recall)
TPR <- cm[2,2]/sum(cm[2,])
# Precision
PRE <- cm[2,2]/sum(cm[,2])
# True negative rate: TNR
TNR <- cm[1,1]/sum(cm[1,])
# Simple Accuracy
ACC <- (cm[1,1]+cm[2,2])/sum(cm)
# Balanced Correction Rate
BCR <- sqrt(TPR*TNR)
# F1-Measure
F1 <- 2*TPR*PRE/(TPR+PRE)
return(c(TPR, PRE, TNR, ACC, BCR, F1))
}
# Initialize the performance matrix
perf_mat <- matrix(0, 1, 6)
colnames(perf_mat) <- c("TPR (Recall)", "Precision", "TNR", "ACC", "BCR", "F1")
rownames(perf_mat) <- "Logstic Regression"
perf_mat[1, ] = perf_eval2(cm)
print(perf_mat)
# Visualizing the Trainig set results
install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
install.packages("caTools")
